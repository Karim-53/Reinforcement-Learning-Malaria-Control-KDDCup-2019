{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO-SimpleLanding(continous)ToKDD.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python [conda env:root]","language":"python","name":"conda-root-py"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h7WrDsPSDC_s","colab_type":"text"},"source":["# LunarLander-v2\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mEhr44P1HqOj","colab_type":"text"},"source":["##Source\n","https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO_continuous.py\n","\n","(and the discrete: https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py )\n","\n","[source spec](https://github.com/nikhilbarhate99/PPO-PyTorch):\n","\n","Python 3.6\n","\n","PyTorch 1.0\n","\n","NumPy 1.15.3\n","\n","gym 0.10.8\n","\n","Pillow 5.3.0"]},{"cell_type":"markdown","metadata":{"id":"jHWkXJpVDC_t","colab_type":"text"},"source":["##description for the discrete prob\n","https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2\n","•Landing pad is always at coordinates (0,0).\n","•Coordinates are the first two numbers in state vector.\n","•Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back.\n","•Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n","Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n","LunarLander-v2 defines \"solving\" as getting average reward of 200 over 100 consecutive trials"]},{"cell_type":"markdown","metadata":{"id":"E1TI4oYOAk6h","colab_type":"text"},"source":["##description for the continous prob\n","https://github.com/openai/gym/wiki/Leaderboard#lunarlandercontinuous-v2\n","\n","•Landing pad is always at coordinates (0,0).\n","Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n","If lander moves away from landing pad it loses reward back.\n","Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.\n","•Action is two real values vector from -1 to +1. First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power. **Engine can't work with less than 50% power (j'espere qu il n a pas implementé qlq chose du genre)**.\n","• Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off.\n"]},{"cell_type":"markdown","metadata":{"id":"7O5slP_rHd66","colab_type":"text"},"source":["#Code"]},{"cell_type":"code","metadata":{"id":"doPYPvB-wK4-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c79d2078-dd53-498d-8e06-a8609b333ada","executionInfo":{"status":"ok","timestamp":1559991953975,"user_tz":-120,"elapsed":870,"user":{"displayName":"sephiros sama","photoUrl":"https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg","userId":"04364851670955414673"}}},"source":["import os\n","\n","try:\n","  import torch\n","  import torch.nn as nn\n","  from torch.distributions import MultivariateNormal\n","  import gym\n","  import numpy as np\n","  from netsapi.challenge import *\n","except:\n","  !pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n","  !pip install box2d-py\n","  print(\"restart the kernel and go\")\n","  os._exit(0)  \n","\n","# for the KDD-env\n","from sys import exit, exc_info, argv\n","from multiprocessing import Pool, current_process\n","import random as rand\n","import json\n","import requests\n","import numpy as np\n","import pandas as pd\n","\n","\n","import statistics\n","from IPython.display import clear_output\n","from contextlib import contextmanager\n","import sys, os\n","@contextmanager\n","def suppress_stdout():\n","    with open(os.devnull, \"w\") as devnull:\n","        old_stdout = sys.stdout\n","        sys.stdout = devnull\n","        try:  \n","            yield\n","        finally:\n","            sys.stdout = old_stdout\n","print(\"import successful\")\n","\n","import tensorflow as tf\n","import math\n","import matplotlib.pyplot as plt"],"execution_count":37,"outputs":[{"output_type":"stream","text":["import successful\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NK85Ia_DDC_u","colab_type":"code","colab":{}},"source":["device = \"cpu\"#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Memory:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","    \n","    def clear_memory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1avzSSqFix6","colab_type":"text"},"source":["###nn.Linear           https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html\n","\n","Args:\n","\n","        in_features: size of each input sample\n","        out_features: size of each output sample\n","        bias: If set to ``False``, the layer will not learn an additive bias.   Default: ``True``\n","        \n","### torch.distributions.multivariate_normal.MultivariateNormal\n","https://pytorch.org/docs/stable/distributions.html#multivariatenormal\n","\n","Parameters\n","\n","         loc (Tensor) – mean of the distribution         \n","         covariance_matrix (Tensor) – positive-definite covariance matrix\n","         precision_matrix (Tensor) – positive-definite precision matrix\n","         scale_tril (Tensor) – lower-triangular factor of covariance, with positive-valued diagonal\n","Ex\n","        m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n","        m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`"]},{"cell_type":"code","metadata":{"id":"NKjd-cy6FhiN","colab_type":"code","colab":{}},"source":["\n","class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, n_var, action_std):\n","        super(ActorCritic, self).__init__()\n","        # action mean range -1 to 1 #TODO how to change this to [0,1]\n","        self.actor =  nn.Sequential(\n","                nn.Linear(state_dim, n_var),\n","                nn.Tanh(),\n","                nn.Linear(n_var, n_var),\n","                nn.Tanh(),\n","                nn.Linear(n_var, action_dim),\n","                nn.Tanh()#todo change this to have something in the  [0, 1] interval ***************\n","                )\n","        # critic\n","        self.critic = nn.Sequential(  #TODO penser a supp ca since we have alredy a well defined reward\n","                nn.Linear(state_dim, n_var),\n","                nn.Tanh(),\n","                nn.Linear(n_var, n_var),\n","                nn.Tanh(),\n","                nn.Linear(n_var, 1)\n","                )\n","        self.action_var = torch.full((action_dim,), action_std*action_std).to(device) # define the variance of the action taken (gaussian model)\n","        self.action_upper_bound = 1.0 #kim\n","        self.action_lower_bound = 0.0 #kim\n","    def forward(self):\n","        raise NotImplementedError\n","    \n","    def act(self, state, memory):\n","        #print(\"act::state \",state)\n","        type(state)\n","        action_mean = self.actor(state)\n","        #print(\"action_mean: \", action_mean)\n","        dist = MultivariateNormal(action_mean, torch.diag(self.action_var).to(device)) # Gaussian distrib used by default by PPO\n","        # To get a bounded action space we use Beta distrib:\n","        # beta(4,4) approximate the gaussian well but still need to work on it ++ https://stats.stackexchange.com/questions/317729/is-the-gaussian-distribution-a-specific-case-of-the-beta-distribution\n","        #dist = Beta(torch.tensor([4.0,4.0]), torch.tensor([4.0,4.0])) # https://pytorch.org/docs/stable/distributions.html#beta\n","        # todo transform action_mean, torch.diag(self.action_var).to(device) --> beta param\n","        action = dist.sample()\n","        \n","        \n","        #print(action)\n","        action = action.tolist()\n","        action = [ (math.tanh(action[0])+1.0)/2.0 , (math.tanh(action[1])+1.0)/2.0  ] #https://pytorch.org/docs/stable/nn.html#tanh\n","        #print(action)\n","        #action+=1\n","        #print(action)\n","        #action/=2.0\n","        #print(action)\n","        action = torch.tensor(action)#this is making me use cpu and not gpu\n","        #print(action)\n","        \n","        \n","        \n","        \n","        action_logprob = dist.log_prob(action)\n","        memory.states.append(state)\n","        memory.actions.append(action)\n","        memory.logprobs.append(action_logprob)\n","        # ++ how to limit my action space to https://github.com/openai/baselines/issues/121\n","        \n","        # kim: i will implement this simple sol and hope it works: https://github.com/openai/baselines/issues/121#issuecomment-369688616\n","        # action = np.clip(action, self.action_space.low, self.action_space.high)\n","        # well since it s not a toch obj i will skip this sol XD --> let's use beta distrib (up) \n","        # --> i don t know how to integrate the param of action_mean and action_var into the beta distrib --> go back to upper bound limit\n","        #todo use a wrapper func for this thing ++ https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/\n","        #action = torch.clamp(action, self.action_lower_bound, self.action_upper_bound, out=None) # https://pytorch.org/docs/master/torch.html?#torch.clamp\n","        #we get a lot of 0 and 1 even if it a good thing but it will not learn in an efficient way since 0 was the mean and we trucate in 0 XD\n","        \n","        \n","        return action.detach()\n","    \n","    def evaluate(self, state, action):\n","        action_mean = self.actor(state)\n","        dist = MultivariateNormal(torch.squeeze(action_mean), torch.diag(self.action_var))\n","        \n","        action_logprobs = dist.log_prob(torch.squeeze(action))\n","        dist_entropy = dist.entropy()\n","        state_value = self.critic(state)\n","        \n","        return action_logprobs, torch.squeeze(state_value), dist_entropy\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim, n_latent_var, action_std, lr, betas, gamma, K_epochs, eps_clip):\n","        self.lr = lr\n","        self.betas = betas\n","        self.gamma = gamma\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","        \n","        self.policy = ActorCritic(state_dim, action_dim, n_latent_var, action_std).to(device)\n","        self.optimizer = torch.optim.Adam(self.policy.parameters(),\n","                                              lr=lr, betas=betas)\n","        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var, action_std).to(device)\n","        \n","        self.MseLoss = nn.MSELoss()\n","    \n","    def select_action(self, state, memory):\n","        #print(\"select_action::state \", state)\n","        # state will not be reshaped (1, -1) since we have only one state -.-\n","        state = torch.FloatTensor(state).to(device) # todo probably have to change it to int\n","        #print(\"select_action:::state \", state)\n","        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n","    \n","    def update(self, memory):\n","        # Monte Carlo estimate of rewards:\n","        rewards = []\n","        discounted_reward = 0\n","        for reward in reversed(memory.rewards):\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","        \n","        # Normalizing the rewards:\n","        rewards = torch.tensor(rewards).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n","        \n","        # convert list to tensor\n","        old_states = torch.stack(memory.states).to(device).detach()\n","        old_actions = torch.stack(memory.actions).to(device).detach()\n","        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\n","        \n","        # Optimize policy for K epochs:\n","        for _ in range(self.K_epochs):\n","            # Evaluating old actions and values :\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n","            # Finding the ratio (pi_theta / pi_theta__old):\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","\n","            # Finding Surrogate Loss:\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            \n","            # take gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","            \n","        # Copy new weights into old policy:\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_p5ILPZiiOd","colab_type":"code","colab":{}},"source":["#creating env of KDD  (extending gym) https://www.novatec-gmbh.de/en/blog/creating-a-gym-environment/\n","from gym import error, spaces, utils\n","from gym.utils import seeding\n"," \n","class KddEnv(gym.Env): \n","    state_dim = 1 # we have only one space dim\n","    action_dim = 2 \n","    #metadata = {'render.modes': ['human']}   \n","    allRewards = []\n","    def __init__(self):\n","        print(\"--> init\")\n","        self.envSeqDec = ChallengeSeqDecEnvironment() #Initialise a New Challenge Environment to post entire policy\n","    def step(self, action): # one action note hole episode\n","        type(action)\n","        type(action.squeeze().tolist() )\n","        s,r,d,_ = self.envSeqDec.evaluateAction(action.squeeze().tolist())\n","        print(\"--> action: \",action,\"    s,r,d,_\", s,\"  \",r,\"  \",d)\n","        self.allRewards.append(r)\n","        return [s],r,d,_\n","    def reset(self):\n","        print(\"--> reset\")\n","        #self.envSeqDec.reset()\n","        self.envSeqDec = ChallengeSeqDecEnvironment()\n","        return [1]\n","    def render(self, mode='human', close=False):\n","        if len(self.allRewards) % 50 == 5 : \n","          plt.plot(self.allRewards)\n","          #plt.xlim((0,120))\n","          plt.ylim((-100,150))\n","          plt.show()\n","          concat = np.add.reduceat(self.allRewards, np.arange(0, len(self.allRewards), 5))\n","          print(np.max(concat))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNNOTwLFiiot","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":659448,"output_embedded_package_id":"1a_lfyK1dATl5LOrOfopNX4JRWnGrCxKp"},"outputId":"54b8c555-6524-4186-ab38-919c67f6749f","executionInfo":{"status":"ok","timestamp":1559996041437,"user_tz":-120,"elapsed":227445,"user":{"displayName":"sephiros sama","photoUrl":"https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg","userId":"04364851670955414673"}}},"source":["def main():\n","    ############## Hyperparameters ##############\n","    env_name = \"KDD-v2\"    #\"LunarLanderContinuous-v2\"\n","    render = True              # juste lel affichage bta3 el model (gym)\n","    solved_reward = 200    #200 # stop training if avg_reward > solved_reward\n","    log_interval = 5       #20  # print avg reward in the interval\n","    max_episodes = 2000#20      #50000# max training episodes\n","    max_timesteps = 5      #300 # max timesteps in one episode\n","    n_latent_var = 16 #8       #64    # number of variables in hidden layer\n","                                # https://stats.stackexchange.com/a/354476    :\n","                                # classic statistical advice to use the number of samples at least 10 times more than the number of parameters. This is vague, of course. If the problem is too noisy, you can demand 100 times more, or 1000 times more.\n","                                # we have a max of 100 cases (actions) or in worst case 20 cases (episode)\n","    update_timestep = 5    #4000# update policy every n timesteps ; nthon lezem max_timesteps\n","    action_std = 0.6 # 0.4       #0.6 # constant std for action distribution   # je sais pas quoi mettre mais je doit faire une hyperparm opt on it\n","                                # my action space is 2 time smaller than the original prob so i devided by sqrt(2)\n","    lr = 0.0025                 # learning rate : https://www.freecodecamp.org/news/how-to-pick-the-best-learning-rate-for-your-machine-learning-project-9c28865039a8/\n","                                # i will have to run a hyperparm opt on it even if it's the best value \n","    betas = (0.9, 0.999)        # est ce que je devrai hyperOpt this or not ??? ++beta\n","    gamma = 0.99                # discount factor  # hyperOpt ++\n","    K_epochs = 5           #5   # update policy for K epochs\n","    eps_clip = 0.2              # clip parameter for PPO\n","    random_seed = None\n","    #############################################\n","    \n","    # creating environment\n","    env = KddEnv()\n","    if random_seed:\n","        print(\"Random Seed: {}\".format(random_seed))\n","        torch.manual_seed(random_seed)\n","        env.seed(random_seed)\n","        np.random.seed(random_seed)\n","        \n","    memory = Memory()\n","    ppo = PPO(env.state_dim, env.action_dim, n_latent_var, action_std, lr, betas, gamma, K_epochs, eps_clip) # this beta have nothing todo with what i'm trying to add\n","    \n","    # logging variables\n","    running_reward = 0\n","    old_running_reward = 0\n","    avg_length = 0\n","    time_step = 0\n","    \n","    # training loop\n","    for i_episode in range(1, max_episodes+1):\n","        state = env.reset()\n","        for t in range(max_timesteps):\n","            time_step +=1\n","            print(\"time_step \",time_step )\n","            # Running policy_old:\n","            action = ppo.select_action(state, memory)\n","            state, reward, done, _ = env.step(action)\n","            # Saving reward:\n","            memory.rewards.append(reward)\n","            \n","            # update if its time\n","            if time_step % update_timestep == 0:\n","                ppo.update(memory)\n","                memory.clear_memory()\n","                time_step = 0\n","            running_reward += reward\n","            if render:\n","                env.render()\n","            if done:\n","                print(\"running_reward (tot) when done: \",running_reward - old_running_reward)\n","                old_running_reward = running_reward\n","                break\n","        \n","        avg_length += t\n","        \n","        # # stop training if avg_reward > solved_reward\n","        if running_reward > (log_interval*solved_reward):\n","            print(\"########## Solved! ##########\")\n","            torch.save(ppo.policy.state_dict(), './PPO_Continuous_{}.pth'.format(env_name))\n","            break\n","        # logging\n","        if i_episode % log_interval == 0:\n","            avg_length = int(avg_length/log_interval)\n","            running_reward = int((running_reward/log_interval))\n","            \n","            print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(i_episode, avg_length, running_reward))\n","            running_reward = 0\n","            avg_length = 0\n","            \n","if __name__ == '__main__':\n","    main()"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}